---
title: 电商广告分析系统
tags: Flume,Kafka,SparkStreaming
grammar_cjkRuby: true
grammar_mindmap: true
renderNumberedHeading: true
---

# 核心需求

1、在线黑名单过滤
2、计算每个BatchDuration中每个User的广告点击量
3、判断用户点击是否属于黑名单点击
4、广告点击累计动态更新
5、对广告点击进行TopN
6、计算过去半个小时广告的点击趋势
7、用户点击数据的收集

# 技术选型
 假设我们的应用系统对应的用户的日活量在百万的基础上来实现此架构。


## Flume与Logstash对比
apache Flume 是一个从可以收集例如日志，事件等数据资源，并将这些数量庞大的数据从各项数据资源中集中起来存储的工具/服务，或者数集中机制。flume具有高可用，分布式，配置工具，其设计的原理也是基于将数据流，如日志数据从各种网站服务器上汇集起来存储到HDFS，HBase等集中存储器中。

![2.1 Flume复杂流结构模型](https://www.github.com/Tu-maimes/document/raw/master/小书匠/多路复杂.jpg)



### Flume的优势


1、Flume可以将应用产生的数据存储到任何集中存储器中，例如HDFS、HBase、Kafka等。
2、当收集数据的速度超过将写入数据的时候，也就是当收集信息遇到峰值时，这时候收集的信息非常大，甚至超过了系统的写入数据的能力，Flume会在数据生产者和数据收容器间做出调整，保证其能够在两者之间提供平稳的数据。
3、Flume的管道是基于事务，保证了数据在传送和接收时的一致性。
4、Flume是可靠的，容错性高，可升级，易管理，并且可定制。
5、支持各种接入资源数据的类型以及输出数据的类型。
6、支持多路径流量，多管道接入流量，多管道输出流量，上下文路由等。
7、可以被水平扩展。


### Flume性能测试

#### 测试环境

##### 硬件

- CPU：Intel(R) Core(TM) i7-6700 CPU @ 3.40GHz（8核）
- 内存：16G

##### 软件

- Flume：1.6.0
- Hadoop：2.6.0-cdh5.5.0
- Kfaka：2.11-0.9.0.1
- JDK：1.8.0_91-b14 64位

##### 测试文件

- 文件大小：107M ，共490010条记录

##### Flume配置

1、Source配置:
> agent.sources.source1.type = spooldir
> agent.sources.source1.spoolDir=/data/flume/dir

2、MemoryChannel配置
>agent.channels.memoryChannel.capacity = 1000000
agent.channels.memoryChannel.transactionCapacity = 1000000
agent.channels.memoryChannel.type=memory

3、FileChannel配置

> agent.channels.fileChannel.type = file
agent.channels.fileChannel.checkpointDir = /data/flume/checkpoint
agent.channels.fileChannel.dataDirs = /data/flume/data

4、JVM配置

>JAVA_OPTS="-Xms256m -Xmx256m -Xss256k -Xmn128m -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:-UseGCOverheadLimit"


#### 写入文件性能

|  FlumeConfig   |Time(s)     |  Throughput（events/s）   |
| --- | --- | --- |
|MemoryChannel+FileSink	|51|	9608|
|FileChannel+FileSink|	250	|1960|

#### 写入Kafka性能

|  FlumeConfig   |Time(s)     |  Throughput（events/s）   |
| --- | --- | --- |
|KafkaSink+MemoryChannel|	57	|8597|
|KafkaChannel	|50|	9800|
|KafkaSink+FileChannel|	830|	590|

#### 写入HDFS性能

|  FlumeConfig   |Time(s)     |  Throughput（events/s）   |
| --- | --- | --- |
|FileChannel+HdfsSink	|148|	3311|



## Logstash简介

Logstash是一个开源数据收集引擎，具有实时管道功能。Logstash可以动态地将来自不同数据源的数据统一起来，并将数据标准化到你所选择的目的地。Logstash常用于日志关系系统中做日志采集设备与Elasticsearch、Kibana，FileBeat一起使用被称为ELK。

[Logstash入门](https://blog.csdn.net/chenleiking/article/details/73563930)
[Filebeat工作原理](https://www.jianshu.com/p/6282b04fe06a)

### 系统架构


 Logstash的事件（logstash将数据流中等每一条数据称之为一个event）处理流水线有三个主要角色完成：inputs –> filters –> outputs：
- inpust：必须，负责产生事件（Inputs generate events），常用：File、syslog、redis、beats（如：Filebeats）
- filters：可选，负责数据处理与转换（filters modify them），常用：grok、mutate、drop、clone、geoip
- outpus：必须，负责数据输出（outputs ship them elsewhere），常用：elasticsearch、file、graphite、statsd

## Flume与Logstash对比

### Flume的特点

1、侧重数据传输，有内部机制确保不会丢数据，用于重要日志场景；
2、由java开发，没有丰富的插件，主要靠二次开发；
3、配置繁琐，堆外暴露监控端口接受数据。

### Logstash的特点

1、内部没有一个persist queue，异常情况可能会丢失部分数据；
2、由ruby编写，需要ruby环境，插件很多；
3、偏重数据前期处理，分析方便。

### 比较

||flume|logstash|
|---|---|---|
|结构上|Source、Channel、Sink	|Shipper、Broker、Indexer|
|简易程度|很繁琐，要分别作source、channel、sink的手工配置，而且涉及到复杂的数据采集环境	|简洁清晰，三个部分的属性都定义好了，只需选择就好，而且可以自行开发插件|
|历史背景|最初设计的目的是为了把数据传入HDFS中，侧重传输(多路由)，重稳定性|	侧重对数据的预处理，因为日志的字段需要大量的预处理，为解析做铺垫|
|对比|像是散装的台式机，使用较麻烦，工具繁多，需要根据业务选择|	更像是组装好的台式机，使用简单方便，搭配ELK更高效|





## Kafka与RibbitMQ对比

### RabbitMQ架构

RabbitMQ是一个分布式系统，这里面有几个抽象的概念。

- broker:每一个节点运行的服务程序，功能为维护改节点的队列的增删以及转发队列操作请求。
- master queue:每个队列都分为一个主队列和若干个镜像队列。
- mirror queue:镜像队列，作为master queue的备份。在master queue所在的节点挂掉之后，系统把mirror queue提升为master queue，负责处理客户端队列操作请求。注意，mirror queue只做镜像，设计目的不是为了承担客户端读写压力。

![RibbitMQ队列模型](https://www.github.com/Tu-maimes/document/raw/master/小书匠/180912132918261.png)

如上图所示，集群中有两个节点，每个节点上有一个broker，每个broker负责本机上队列的维护，并且borker之间可以互相通信。集群中有两个队列A和B，每个队列都分为master queue和mirror queue（备份）。

![RibbitMQ消费模型](https://www.github.com/Tu-maimes/document/raw/master/小书匠/180912132918262.png)

如上图有两个consumer消费队列A，这两个consumer连在了集群的不同机器上。RabbitMQ集群中的任何一个节点都拥有集群上所有队列的元信息，所以连接到集群中的任何一个节点都可以，主要区别在于有的consumer连在master queue所在节点，有的连在非master queue节点上。

因为mirror queue要和master queue保持一致，故需要同步机制，正因为一致性的限制，导致所有的读写操作都必须都操作在master queue上（想想，为啥读也要从master queue中读？和数据库读写分离是不一样的。），然后由master节点同步操作到mirror queue所在的节点。即使consumer连接到了非master queue节点，该consumer的操作也会被路由到master queue所在的节点上，这样才能进行消费。


![RibbitMQ生产模型](https://www.github.com/Tu-maimes/document/raw/master/小书匠/180912132918263.png)

原理和消费一样，如果连接到非 master queue 节点，则路由过去。所以可以看到 RabbitMQ的不足：由于master queue单节点，导致性能瓶颈，吞吐量受限。虽然为了提高性能，内部使用了Erlang这个语言实现，但是终究摆脱不了架构设计上的致命缺陷。


### Kafka架构

Kafka我觉得就是看到了RabbitMQ这个缺陷才设计出的一个改进版，改进的点就是：把一个队列的单一master变成多个master，即一台机器扛不住qps，那么我就用多台机器扛qps，把一个队列的流量均匀分散在多台机器上不就可以了么？注意，多个master之间的数据没有交集，即一条消息要么发送到这个master queue，要么发送到另外一个master queue。

这里面的每个master queue 在Kafka中叫做Partition，即一个分片。一个队列有多个主分片，每个主分片又有若干副分片做备份，同步机制类似于RabbitMQ。

![Kafka生产模型](https://www.github.com/Tu-maimes/document/raw/master/小书匠/180912132918265.png)

如上图，我们省略了不同的queue，假设集群上只有一个queue（Kafka中叫Topic）。每个生产者随机把消息发送到主分片上，之后主分片再同步给副分片。

![Kafka消费模型](https://www.github.com/Tu-maimes/document/raw/master/小书匠/180912132918264.png)


队列读取的时候虚拟出一个Group的概念，一个Topic内部的消息，只会路由到同Group内的一个consumer上，同一个Group中的consumer消费的消息是不一样的；Group之间共享一个Topic，看起来就是一个队列的多个拷贝。所以，为了达到多个Group共享一个Topic数据，Kafka并不会像RabbitMQ那样消息消费完毕立马删除，而是必须在后台配置保存日期，即只保存最近一段时间的消息，超过这个时间的消息就会从磁盘删除，这样就保证了在一个时间段内，Topic数据对所有Group可见（这个特性使得Kafka非常适合做一个公司的数据总线）。队列读同样是读主分片，并且为了优化性能，消费者与主分片有一一的对应关系，如果消费者数目大于分片数，则存在某些消费者得不到消息。

### Kafka的优势


 消息系统的特点：生存者消费者模型，先入先出（FIFO）

• 高性能：单节点支持上千个客户端，高吞吐量

1.     [零拷贝技术](https://mp.weixin.qq.com/s?__biz=MzI4NTA1MDEwNg==&mid=2650778475&idx=1&sn=17491e35e597b9f96a1848dde5eeb6ba&chksm=f3f91efec48e97e8bb0fadf0f574ee06c90e510156d20117985610fbadc8c61a1b2b3a7e9121&mpshare=1&scene=1&srcid=&key=e8b343f779d182b90524c7b06e9f1e8c90c7d304f021337b0d5ac467708ebf35f28b9a7100f775af0d520b6b3b0fe2c10feff809a9bba056c154ceca77794c2026d9bf03d02bb5da4f67427f70c1fed0&ascene=1&uin=MTEyNDQ3MDMyMQ%3D%3D&devicetype=Windows+10&version=62060833&lang=zh_CN&pass_ticket=10HxXi3v%2F1ZN5pVLImV%2Fd9K9LDtNsjOrAfvUn9j1vT2I25OIwmR5MXude6PJxJae)

2.     分布式存储

3.     顺序读顺序写

4.     批量读批量写

• 持久性：消息直接持久化在普通磁盘上，且性能好

• 分布式：数据副本冗余、流量负载均衡、可扩展

• 很灵活：消息长时间持久化+Client维护消费状态

注意：消息系统基本的特点是保证了，有基本的生产者消费者模型，partition内部是FIFO 的，partition之间不是FIFO，当然我们可以把topic设为一个partition，这样就严格的FIFO

### Kafka性能测试

[参考文档](https://www.cnblogs.com/smartloli/p/10093838.html)

#### 测试环境

本次测试的环境信息由三台物理机组成，具体信息：

|主机名|Kafka版本|CPU|内存|磁盘|网卡|
| --- | --- | --- | --- | --- | --- |
|dn1|0.10.2.0|32核|64GB|12x4T|千兆|
|dn2|0.10.2.0|32核|64GB|12x4T|千兆|
|dn3|0.10.2.0|32核|64GB|12x4T|千兆|

#### 生产者测试

生产者测试，分别从线程数、分区数、副本数、Broker数、同步与异步模式、批处理大小、消息长度大小、数据压缩等。

##### 线程数
![](https://www.github.com/Tu-maimes/document/raw/master/小书匠/1561030637347.png)

结论：向一个拥有6个分区、1个副本的Topic中，发送500万条消息记录时，随着线程数的增加，每秒发送的消息记录会逐渐增加。在线程数为25时，每秒发送的消息记录达到最佳值，随后再增加线程数，每秒发送的消息记录数反而会减少。

##### 分区数

![](https://www.github.com/Tu-maimes/document/raw/master/小书匠/1561031339014.png)

总结：从测试结果来看，分区数越多，单线程生产者的吞吐量越小。

##### 副本数

![](https://www.github.com/Tu-maimes/document/raw/master/小书匠/1561079067158.png)

总结：从测试结果来看，副本数越多，吞吐量越小。

##### Broker数量

![](https://www.github.com/Tu-maimes/document/raw/master/小书匠/1561079171982.png)

总结：从测试结果来看，增加Kafka Broker数量，吞吐量会增加。

##### 同步与异步模式

![](https://www.github.com/Tu-maimes/document/raw/master/小书匠/1561079326258.png)

总结：从测试结果来看，使用异步模式发送消息数据，比使用同步模式发送消息数据，吞吐量是同步模式的3倍左右。

##### 批处理大小

![](https://www.github.com/Tu-maimes/document/raw/master/小书匠/1561079604995.png)

总结：从测试的结果来看，发送的消息随着批处理大小增加而增加。当批处理大小增加到3000~5000时，吞吐量达到最佳状态，而后在增加批处理大小，吞吐量的性能会下降。

##### 消息长度的大小

![](https://www.github.com/Tu-maimes/document/raw/master/小书匠/1561080132100.png)

总结：从测试结果来看，随着消息长度的增加，每秒所能发送的消息逐渐减少（nMsg/sec）。但是，每秒发送的消息的总大小（MB/sec),会随着消息长度的增加而增加。

#### Kafka消费者测试

消费者测试，可以从线程数、分区数、副本数等维度来进行测试。

##### 线程数

![](https://www.github.com/Tu-maimes/document/raw/master/小书匠/1561080799340.png)

总结：随着线程数的增加，每秒读取的消息记录会逐渐增加。在线程数与消费主题的分区相等时，吞吐量达到最佳值。随后，再增加线程数，新增的线程数将会处于空闲状态，对提升消费者程序的吞吐量没有帮助。

##### 分区数

![](https://www.github.com/Tu-maimes/document/raw/master/小书匠/1561081154469.png)

总结：当分区数增加时，如果线程数保持不变，则消费者程序的吞吐量性能会下降。

##### 副本数

![](https://www.github.com/Tu-maimes/document/raw/master/小书匠/1561081268274.png)

总结：副本数对消费者程序的吞吐量影响较小，消费者程序是从Topic的每个分区的Leader上读取数据的，而与副本数无关。 

## SparkStreaming简介

Spark Streaming是核心Spark API的扩展，可实现实时数据流的可扩展，高吞吐量，容错流处理。数据可以从许多来源（如Kafka，Flume，Kinesis或TCP套接字）中提取，并且可以使用以高级函数表示的复杂算法进行处理map，例如reduce，join和window。最后，处理后的数据可以推送到文件系统，数据库和实时仪表板。

![SparkStreaming体系架构](https://www.github.com/Tu-maimes/document/raw/master/小书匠/1561083559745.png)

### 检查点

- 元数据检查点 - 将定义流式计算的信息保存到容错存储（如HDFS）。这用于从运行流应用程序的驱动程序的节点的故障中恢复（稍后详细讨论）。元数据包括：
 1、配置 - 用于创建流应用程序的配置。
 2、DStream操作 - 定义流应用程序的DStream操作集。
 3、不完整的批次 - 其工作排队但尚未完成的批次。
- 数据检查点 - 将生成的RDD保存到可靠的存储。在一些跨多个批次组合数据的有状态转换中，这是必需的。在这种转换中，生成的RDD依赖于先前批次的RDD，这导致依赖关系链的长度随时间增加。为了避免恢复时间的这种无限增加（与依赖链成比例），有状态变换的中间RDD周期性地 检查点到可靠存储（例如HDFS）以切断依赖链。
总而言之，元数据检查点主要用于从驱动程序故障中恢复，而如果使用状态转换，即使对于基本功能也需要数据或RDD检查点。

#### 启用检查点

必须为具有以下任何要求的应用程序启用检查点：

- 有状态转换的用法 - 如果在应用程序中使用了（updateStateByKey或reduceByKeyAndWindow使用反函数），则必须提供检查点目录以允许定期RDD检查点。
- 从运行应用程序的驱动程序的故障中恢复 - 元数据检查点用于使用进度信息进行恢复。


请注意，可以在不启用检查点的情况下运行没有上述有状态转换的简单流应用程序。在这种情况下，驱动程序故障的恢复也将是部分的（某些已接收但未处理的数据可能会丢失）。这通常是可以接受的，并且许多以这种方式运行Spark Streaming应用程序。预计对非Hadoop环境的支持将在未来得到改善。

#### 配置检查点

可以通过在容错，可靠的文件系统（例如，HDFS，S3等）中设置目录来启用检查点，检查点信息将保存到该文件系统中。这是通过使用完成的streamingContext.checkpoint(checkpointDirectory)。这将允许您使用上述有状态转换。此外，如果要使应用程序从驱动程序故障中恢复，则应重写流应用程序以使其具有以下行为。

- 当程序第一次启动时，它将创建一个新的StreamingContext，设置所有流然后调用start（）。
- 当程序在失败后重新启动时，它将从检查点目录中的检查点数据重新创建StreamingContext。

### SparkStreaming与Kafka的集成

Kafka 0.10的Spark Streaming集成它提供简单的并行性，Kafka分区和Spark分区之间的1：1对应关系，以及对偏移和元数据的访问。
[Spark官网集成文档](http://spark.apache.org/docs/latest/streaming-kafka-0-10-integration.html)
[两种接受方式比较](https://www.cnblogs.com/liugh/articles/6817553.html)

SparkStreaming使用NoReceivers方式读取Kafka数据	
	SparkStreaming读取Kafka数据支撑有两种方式：Receiver方式和NoReceiver方式
	1、Receiver方式：SparkStreamingKafkaUtil使用createStream方法
	2、NoReceivers方式：SparkStreamingKafkaUtil使用createDirectStream方法
	NOReceivers方式在企业中使用的越来越多，具有更强的自由度控制、语义一致性。NO Receivers
	更符合数据读取和数据操作，是我们操作数据来源的自然方式。
	
采用No Receivers 方式直接抓取Kafka数据带来的好处：
	1、No Receivers方式直接抓取Kafka的数据，没有缓存，不会出现内存溢出的问题。如果使用kafakReceiver方式读取数据，会存在内存的问题。需要设置kafka Receiver读取的频率和block Interval等信息
	2、如果采用Receivers方式，Receivers默认情况需要和Worker的Executor绑定，不方便做分布式。如果采用No Receivers direct方式，默认情况下数据会在多个Worker上的Executor，数据天然就是分布式，默认分布在多个Executor上。而Receivers方式就不方便计算。
	3、数据消费的问题，在实际操作的时候采用Receivers的方式有一个弊端，消费数据来不及处理，如果延迟多次SparkStreaming程序就有可能崩溃。但是是采用No REceivers Direct方式访问Kafka数据，就不会存在此问题。因为NO Receivers direct方式直接读取Kafka数据，如果数据有延迟delay，那就不进行下一个处理，因此，No Receivers direct方式就不会存在来不及消费、程序崩溃的问题。
	4、No Receivers direct方式实现完全的语义一致性，不会重复消费数据，而且保证数据一定被消费。No Receivers direct方式与Kafka进行交互，只有数据真正执行成功后才会记录下来。
	
采用 No Receivers方式
	1、无接收器：createDirectStream方式不使用任何接收器，直接从Kafka进群进行查询(新版本已取消Receiver)
	2、偏移量：createDirectStream方式不使用Zookeeper存储偏移量。消费的偏移量由Stream本身进行跟踪记录。Kafka的监控依赖于Zookeeper，为了监控Stream的消费信息，通过获取偏移量更新Kafka、Zookeeper的偏移量。
	3、容错恢复：如果是SparkDirver容错恢复，可以在StreamingContext中启用检查点。消费的偏移量可以从检查点恢复。
	4、端到端的语义：确保每个记录有效接收和转换一次，但不保证转换以后的数据只输出一次。
	

### Kafka偏移量存储

- 检查点
- Kafka本身
- 自定义存储


# 电商广告点击系统架构

本架构主要是通过分布式Flume采集web系统的数据汇聚到Kafka消息系统，以及对数据实现备份。Kafak消息系统中的数据提供给SparkStreaming进行流式处理，处理的结果存储到数据库中供其他系统使用。


![架构图](https://www.github.com/Tu-maimes/document/raw/master/小书匠/Spark流处理.jpg)