---
title: Logstash文档
tags: 作者:汪帅
grammar_cjkRuby: true
grammar_mindmap: true
renderNumberedHeading: true
---

[toc!?direction=lr]


[Logstash的介绍](https://www.cnblogs.com/cjsblog/p/9459781.html)

Logstash管道有两个必需的元素，输入和输出，以及一个可选元素过滤器。输入插件从数据源那里消费数据，过滤器插件根据你的期望修改数据，输出插件将数据写入目的地。


![架构模型](https://www.github.com/Tu-maimes/document/raw/master/小书匠/1546827759925.png)


# LogStash的安装

 1. [LogStash的下载地址](https://www.elastic.co/downloads/logstash)
 2. 安装JDK1.8
 3. 上传下载的安装包到/opt/logstash目录下，解压`tar -zxvf  logstash-6.5.4.tar.gz`
 4. 配置全局环境变量`vi /etc/profile`添加如下内容

``` shell?linenums
export JAVA_HOME=/usr/java/jdk1.8.0_152
export LOGSTASH_HOME=/opt/logstash/logstash-6.5.4
export CLASSPATH=.:$JAVA_HOME/jre/lib/rt.jar:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar
export PATH=$PATH:$JAVA_HOME/bin:$LOGSTASH_HOME/bin
```

 5. 查看是否安装成功：`logstash -V`
显示所安装的版本号：logstash 6.5.4
 6. 运行第一个程序测试：

``` bash
logstash -e'input {stdin {}} output {stdout {}}'
```
启动后再命令行输入Hello Word! 回车，在控制台会显示

``` shell?linenums
{
      "@version" => "1",
       "message" => "Hello Word! ",
          "host" => "master",
    "@timestamp" => 2019-01-07T05:23:53.238Z
}
```

 7. Ctrl+C可以停止！

# 常用插件

## File input plugin


通常，日志记录会在每行写入的末尾添加换行符。默认情况下，假定每个事件都是一行，并且一行被视为换行符之前的文本。如果您想将多个日志行连接到一个事件中，您将需要使用多行编解码器。插件在发现新文件和处理每个发现的文件之间循环。发现的文件具有生命周期，它们从“观察”或“忽略”状态开始。生命周期中的其他状态是：“主动”，“关闭”和“未观看”。

默认情况下，使用4095个文件的窗口来限制正在使用的文件句柄数。处理阶段有许多阶段：
- 检查自上次以来“关闭”或“忽略”文件的大小是否已更改，如果是，则将其置于“已观看”状态。
- 选择足够的“监视”文件来填充窗口中的可用空间，这些文件变为“活动”。
- 打开并读取活动文件，默认情况下，每个文件从最后的已知位置读取到当前内容（EOF）的末尾。

文件的读取策略：

 1. 在某些情况下，能够控制首先读取哪些文件，排序以及文件是完全读取还是带状/条带化是很有用的。
 2. 完全读取的是文件A，然后是文件B，然后是文件C，等等。
 3. 带状或带状读取是先读取文件A，然后读取文件B，然后读取文件C，如此循环，再读取文件A，直到读取所有文件

带状读取是通过更改file_chunk_count或者file_chunk_size指定的。如果您希望所有文件中的一些事件尽可能早地出现在Kibana中，那么绑定和排序可能非常有用。

### 该插件的两种模式Tail模式与Read模式

#### Tail模式

在这种模式下，插件的目标是跟踪更改的文件，并在添加到每个文件时发出新的内容。在这种模式下，文件被视为一个永不结束的内容流，EOF没有特殊的意义。插件总是假设会有更多的内容。当文件被旋转时，检测到较小或为零的大小，当前位置被重置为零并继续流。必须先看到分隔符，然后才能将累积的字符发出一行。

 1. 在Tail模式下的文件

无论文件是通过重命名还是复制操作旋转，都可以通过此输入检测和处理文件旋转。要支持在轮换发生后写入旋转文件一段时间的程序，请在文件名模式中包含原始文件名和轮换文件名（例如/ var / log / syslog和/var/log/syslog.1）观看（path选项）。对于一个重命名，索引节点将作为从已移动被检测 /var/log/syslog到/var/log/syslog.1因此“状态”也在内部移动，旧内容将不会被重读，但重命名文件中的任何新内容都将被读取。对于复制/截断复制的内容到新文件路径（如果发现），将被视为新发现并从头开始读取。因此，复制的文件路径不应该在要监视的文件名模式中（path选项）。将检测到截断并将“最后读取”位置更新为零。


#### Read模式

在这种模式下，插件将每个文件视为内容完整，即有限的行流，现在EOF非常重要。不需要最后一个分隔符，因为EOF意味着可以将累积的字符作为一行发出。此外，这里的EOF意味着可以关闭文件并将其置于“未监视”状态——这将自动释放活动窗口中的空间。这种模式还可以处理内容完整的压缩文件。读取模式还允许在完全处理文件之后执行操作。

与Flume的区别：在过去尝试模拟读模式同时仍然假设无限流并不理想，专用读模式是一种改进。


### 对监视文件读取的当前位置上


该插件通过在一个名为sincedb的单独文件中记录当前位置来跟踪每个文件中的当前位置。这使得停止和重新启动Logstash成为可能，并让它在不丢失在停止Logstash时添加到文件中的行数的情况下，从停止的地方继续运行。

默认情况下，sincedb文件被放置在Logstash的数据目录中，文件名基于所监视的文件名模式(即path选项)。因此，更改文件名模式将导致使用一个新的sincedb文件，并且将丢失任何现有的当前位置状态。如果您以任意频率更改模式，那么使用sincedb_path选项显式地选择一个sincedb路径可能是有意义的。

通过标识符跟踪文件。该标识符由inode，主设备号和次设备号组成。在Windows中，从kernel32API调用中获取不同的标识符。
Sincedb记录现在可以过期，这意味着在一段时间后不会记住旧文件的读取位置。文件系统可能需要重用新内容的inode。理想情况下，我们不会使用旧内容的读取位置，但我们没有可靠的方法来检测发生了inode重用。这与读取模式更相关，其中在sincedb中跟踪了大量文件。但请记住，如果记录已过期，将再次读取先前看到的文件。


Sincedb文件是四列、五列或六列的文本文件:

 1. node编号(或等效的)。
 2. 文件系统的主要设备号(或同等设备)。
 3. 文件系统的次要设备号(或等效设备号)。
 4. 文件中的当前字节偏移量。
 5. 最后一个活动时间戳(浮点数)。
 6. 此记录匹配的最后一个已知路径(对于转换为新格式的旧sincedb记录，该路径为空)。

### 从远程网络卷读取


文件输入未在远程文件系统（如NFS，Samba，s3fs-fuse等）上进行全面测试，但偶尔会测试NFS。远程FS客户端给出的文件大小用于控制在任何给定时间读取的数据量，以防止读取已分配但尚未填充的内存。由于我们在标识符中使用设备major和minor来跟踪文件的“最后读取”位置，并且在重新装入设备主要和次要可以更改时，sincedb记录可能在重新安装时不匹配。读取模式可能不适用于远程文件系统，因为客户端发现时的文件大小可能与远程端的文件大小不同，这是由于远程客户端复制过程中的延迟。

### 文件输入配置选项

|设置|输入类型|是否必须|
|-|-|-|
|close_older|number or string_duration|NO|
|delimiter|string|NO|
|discover_interval|number|NO|
|exclude|array|NO|
|file_chunk_count|number|NO|
|file_chunk_size|number|NO|
|file_completed_action|string, one of ["delete", "log", "log_and_delete"]|NO|
|file_completed_log_path|string|NO|
|file_sort_by|string, one of ["last_modified", "path"]|NO|
|file_sort_direction|string, one of ["asc", "desc"]|NO|
|ignore_older|number or string_duration|NO|
|max_open_files|number|NO|
|mode|string, one of ["tail", "read"]|NO|
|path|array|Yes|
|sincedb_clean_after|number or string_duration|NO|
|sincedb_path|string|NO|
|sincedb_write_interval|number or string_duration|NO|
|start_position|string, one of ["beginning", "end"]|NO|
|stat_interval|number or string_duration|NO|

 1. close_older
- 值类型是number或string_duration
- 默认值是“1 hour”

文件输入将关闭上次读取指定持续时间的任何文件（如果指定了数字，则为秒）。这具有不同的含义，具体取决于文件是否被加尾或读取。如果拖尾，并且传入数据中存在大的时间间隔，则可以关闭文件（允许打开其他文件），但是在检测到新数据时将排队等待重新打开。如果读取，文件将在从最后一个字节被读取后的closed_older秒后关闭。

 2. delimiter
- 值类型是字符串
- 默认值是“\n”

设置新行分隔符，默认为“\ n”。请注意，在读取压缩文件时，不使用此设置，而是使用标准的Windows或Unix行结尾。

 3. discover_interval
- 值类型是数字
- 默认值是15

我们多长时间扩展path选项中的文件名模式以发现要观看的新文件。该值是倍数stat_interval，例如，如果stat_interval是“500 ms”，则可以每15 X 500毫秒 - 7.5秒发现新文件文件。在实践中，这将是最好的情况，因为需要考虑阅读新内容所花费的时间。

 4. exclude

- 值类型是数组
- 此设置没有默认值

排除（与文件名匹配，而不是完整路径）。文件名模式在这里也是有效的。
例如，如果你有path =>“/ var / log / *”

在Tail模式下，您可能希望排除gzip压缩文件：
exclude =>“* .gz”

 5. file_chunk_count
 - 值类型是数字
 - 默认值是4611686018427387903

当与file_chunk_size组合时，该选项设置在移动到下一个活动文件之前从每个文件读取多少块(带或条纹)。例如，file_chunk_count为32和file_chunk_size 32KB将处理每个活动文件中的下一个1MB。由于默认值非常大，因此在移动到下一个活动文件之前，该文件被有效地读入EOF。

 6. file_chunk_size

- 值类型是数字
- 默认值为32768(32KB)

以块或块的形式从磁盘读取文件内容，并从块中提取行。请参见file_chunk_count以了解为什么以及何时从默认设置更改此设置。

7. file_completed_action
- 值可以任何的：delete；log；log_and_delete
- 默认是delete

在read模式下，完成文件时应执行什么操作。如果删除指定那么该文件将被删除。如果指定了log，则文件的完整路径将记录到file_completed_log_path设置中指定的文件中 。如果log_and_delete指定，则执行上述两个操作。

 8. file_completed_log_path

- 值类型是字符串
- 此设置没有默认值

应该将完全读取的文件路径添加到哪个文件中。只有当file_completed_action是log或log_and_delete时，才将此路径指定到文件。重要提示:此文件仅追加到-它可以变得非常大。你负责文件的轮换

 9. file_sort_by
- 值可以是任何的：last_modified，path
- 默认是last_modified

应该使用“监视”文件的哪个属性对其进行排序。可以按修改日期或完整路径字母对文件进行排序。以前，已发现并因此“监视”的文件的处理顺序取决于操作系统。

 10. file_sort_direction

- 值可以是任何的：asc；desc
- 默认是asc